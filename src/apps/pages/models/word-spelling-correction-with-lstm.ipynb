{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3976,"sourceType":"datasetVersion","datasetId":2367}],"dockerImageVersionId":28772,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd \nimport os\nprint(os.listdir(\"../input\"))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-05T20:47:35.286320Z","iopub.execute_input":"2024-07-05T20:47:35.286626Z","iopub.status.idle":"2024-07-05T20:47:35.302980Z","shell.execute_reply.started":"2024-07-05T20:47:35.286576Z","shell.execute_reply":"2024-07-05T20:47:35.302040Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"['unigram_freq.csv']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df  = pd.read_csv('../input/unigram_freq.csv')\nprint(df.shape)\ndf.dropna(axis=0,how='any')\nprint(df.shape)\n","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2024-07-05T20:47:35.305124Z","iopub.execute_input":"2024-07-05T20:47:35.305421Z","iopub.status.idle":"2024-07-05T20:47:35.682898Z","shell.execute_reply.started":"2024-07-05T20:47:35.305339Z","shell.execute_reply":"2024-07-05T20:47:35.682020Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"(333333, 2)\n(333333, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"lines = [x for x in df['word'] if type(x) == type('a') ]\nprint(\"Line Count:\",len(lines))\nprint(lines[:4])","metadata":{"execution":{"iopub.status.busy":"2024-07-05T20:47:35.684276Z","iopub.execute_input":"2024-07-05T20:47:35.684516Z","iopub.status.idle":"2024-07-05T20:47:35.796101Z","shell.execute_reply.started":"2024-07-05T20:47:35.684476Z","shell.execute_reply":"2024-07-05T20:47:35.795386Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Line Count: 333331\n['the', 'of', 'and', 'to']\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\ndef process(sent):\n    sent=sent.lower()\n    sent=re.sub(r'[^0-9a-zA-Z ]','',sent)\n    sent=sent.replace('\\n','')\n    return sent    ","metadata":{"execution":{"iopub.status.busy":"2024-07-05T20:47:35.797390Z","iopub.execute_input":"2024-07-05T20:47:35.797633Z","iopub.status.idle":"2024-07-05T20:47:35.805568Z","shell.execute_reply.started":"2024-07-05T20:47:35.797592Z","shell.execute_reply":"2024-07-05T20:47:35.804842Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"lines =[process(x) for x in lines]\ntemp = []\nfor line in lines:\n    temp+= [ x for x in line.split() ]\nlines = list(set(temp))\nprint(\"\\n\".join(lines[:4]))\nprint(\"Number of items:\",len(lines))","metadata":{"execution":{"iopub.status.busy":"2024-07-05T20:47:35.808851Z","iopub.execute_input":"2024-07-05T20:47:35.809197Z","iopub.status.idle":"2024-07-05T20:47:36.847937Z","shell.execute_reply.started":"2024-07-05T20:47:35.809143Z","shell.execute_reply":"2024-07-05T20:47:36.847094Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"portname\nglassblower\ngijimaast\nhypermobility\nNumber of items: 333331\n","output_type":"stream"}]},{"cell_type":"code","source":"char_set = list(\" abcdefghijklmnopqrstuvwxyz0123456789\")\nchar2int = { char_set[x]:x for x in range(len(char_set)) }\nint2char = { char2int[x]:x for x in char_set }\nprint(char2int)\nprint(int2char)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T20:47:36.849739Z","iopub.execute_input":"2024-07-05T20:47:36.850045Z","iopub.status.idle":"2024-07-05T20:47:36.855633Z","shell.execute_reply.started":"2024-07-05T20:47:36.849990Z","shell.execute_reply":"2024-07-05T20:47:36.854860Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '0': 27, '1': 28, '2': 29, '3': 30, '4': 31, '5': 32, '6': 33, '7': 34, '8': 35, '9': 36}\n{0: ' ', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 27: '0', 28: '1', 29: '2', 30: '3', 31: '4', 32: '5', 33: '6', 34: '7', 35: '8', 36: '9'}\n","output_type":"stream"}]},{"cell_type":"code","source":"count = len(char_set)\ncodes = [\"\\t\",\"\\n\",'#']\nfor i in range(len(codes)):\n    code = codes[i]\n    char2int[code]=count\n    int2char[count]=code\n    count+=1\nprint(char2int)\nprint(int2char)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T20:47:36.856948Z","iopub.execute_input":"2024-07-05T20:47:36.857241Z","iopub.status.idle":"2024-07-05T20:47:36.866162Z","shell.execute_reply.started":"2024-07-05T20:47:36.857191Z","shell.execute_reply":"2024-07-05T20:47:36.865405Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '0': 27, '1': 28, '2': 29, '3': 30, '4': 31, '5': 32, '6': 33, '7': 34, '8': 35, '9': 36, '\\t': 37, '\\n': 38, '#': 39}\n{0: ' ', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 27: '0', 28: '1', 29: '2', 30: '3', 31: '4', 32: '5', 33: '6', 34: '7', 35: '8', 36: '9', 37: '\\t', 38: '\\n', 39: '#'}\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\ndef gen_gibberish(line,thresh=0.2):\n    times = int(random.randrange(1,len(line)) * thresh)\n    '''\n    Types of replacement:\n        1.Delete random character.\n        2.Add random character.\n        3.Replace a character.\n        4.Combination?\n    '''\n    while times!=0:\n        times-=1\n        val = random.randrange(0,10)\n        if val <= 5:\n            val = random.randrange(0,10)\n            index = random.randrange(2,len(line))\n            if val <= 3 :\n                line = line[:index]+line[index+1:]\n            else:\n                insert_index = random.randrange(0,len(char_set))\n                line = line[:index] + char_set[insert_index] + line[index:]\n        else:\n            index = random.randrange(0,len(char_set))\n            replace_index = random.randrange(2,len(line))\n            line = line[:replace_index] + char_set[index] + line[replace_index+1:]\n    return line\n\nsample = lines[5]\ngib = gen_gibberish(sample)\nprint(\"Original:\",sample)\nprint(\"Gibberish:\",gib)\n        \n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-05T20:47:36.867738Z","iopub.execute_input":"2024-07-05T20:47:36.868067Z","iopub.status.idle":"2024-07-05T20:47:36.880529Z","shell.execute_reply.started":"2024-07-05T20:47:36.868009Z","shell.execute_reply":"2024-07-05T20:47:36.879561Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Original: mycena\nGibberish: mycena\n","output_type":"stream"}]},{"cell_type":"code","source":"input_texts = []\ntarget_texts = []\nREPEAT_FACTOR = 1\nSKIP = int(len(lines)*0.65)\n\nfor line in lines[SKIP:]:\n    if len(line)>10:\n        output_text = '\\t' + line + '\\n'\n        for _ in range(REPEAT_FACTOR):\n            input_text = gen_gibberish(line)\n            input_texts.append(input_text)\n            target_texts.append(output_text)\nprint(\"LEN OF SAMPLES:\",len(input_texts))","metadata":{"execution":{"iopub.status.busy":"2024-07-05T20:47:36.882024Z","iopub.execute_input":"2024-07-05T20:47:36.882304Z","iopub.status.idle":"2024-07-05T20:47:37.098114Z","shell.execute_reply.started":"2024-07-05T20:47:36.882254Z","shell.execute_reply":"2024-07-05T20:47:37.097341Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"LEN OF SAMPLES: 15153\n","output_type":"stream"}]},{"cell_type":"code","source":"max_enc_len = max([len(x) for x in input_texts])\nmax_dec_len = max([len(x) for x in target_texts])\nprint(\"Max Enc Len:\",max_enc_len)\nprint(\"Max Dec Len:\",max_dec_len)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T20:47:37.099383Z","iopub.execute_input":"2024-07-05T20:47:37.099688Z","iopub.status.idle":"2024-07-05T20:47:37.109584Z","shell.execute_reply.started":"2024-07-05T20:47:37.099632Z","shell.execute_reply":"2024-07-05T20:47:37.108697Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Max Enc Len: 42\nMax Dec Len: 40\n","output_type":"stream"}]},{"cell_type":"code","source":"num_samples = len(input_texts)\nencoder_input_data = np.zeros( (num_samples , max_enc_len , len(char_set)),dtype='float32' )\ndecoder_input_data = np.zeros( (num_samples , max_dec_len , len(char_set)+2),dtype='float32' )\ndecoder_target_data = np.zeros( (num_samples , max_dec_len , len(char_set)+2),dtype='float32' )\nprint(\"CREATED ZERO VECTORS\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T20:47:37.111082Z","iopub.execute_input":"2024-07-05T20:47:37.111460Z","iopub.status.idle":"2024-07-05T20:47:37.120782Z","shell.execute_reply.started":"2024-07-05T20:47:37.111404Z","shell.execute_reply":"2024-07-05T20:47:37.119986Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"CREATED ZERO VECTORS\n","output_type":"stream"}]},{"cell_type":"code","source":"for i,(input_text,target_text) in enumerate(zip(input_texts,target_texts)):\n    for t,char in enumerate(input_text):\n        encoder_input_data[ i , t , char2int[char] ] = 1\n    for t,char in enumerate(target_text):\n        decoder_input_data[ i, t , char2int[char] ] = 1\n        if t > 0 :\n            decoder_target_data[ i , t-1 , char2int[char] ] = 1\nprint(\"COMPLETED...\")    ","metadata":{"execution":{"iopub.status.busy":"2024-07-05T20:47:37.122382Z","iopub.execute_input":"2024-07-05T20:47:37.122748Z","iopub.status.idle":"2024-07-05T20:47:37.655973Z","shell.execute_reply.started":"2024-07-05T20:47:37.122665Z","shell.execute_reply":"2024-07-05T20:47:37.655088Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"COMPLETED...\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input,LSTM,Dense","metadata":{"execution":{"iopub.status.busy":"2024-07-05T20:47:37.657403Z","iopub.execute_input":"2024-07-05T20:47:37.657757Z","iopub.status.idle":"2024-07-05T20:47:37.763189Z","shell.execute_reply.started":"2024-07-05T20:47:37.657696Z","shell.execute_reply":"2024-07-05T20:47:37.762440Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 128\nepochs = 1000\nlatent_dim = 256\nnum_enc_tokens = len(char_set)\nnum_dec_tokens = len(char_set) + 2 # includes \\n \\t\nencoder_inputs = Input(shape=(None,num_enc_tokens))\nencoder = LSTM(latent_dim,return_state=True)\nencoder_outputs , state_h, state_c = encoder(encoder_inputs)\nencoder_states = [state_h,state_c]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T20:47:37.764499Z","iopub.execute_input":"2024-07-05T20:47:37.764774Z","iopub.status.idle":"2024-07-05T20:47:38.114933Z","shell.execute_reply.started":"2024-07-05T20:47:37.764724Z","shell.execute_reply":"2024-07-05T20:47:38.114166Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"decoder_inputs = Input(shape=(None,num_dec_tokens))\ndecoder_lstm = LSTM(latent_dim,return_sequences=True,return_state=True)\ndecoder_ouputs,_,_ = decoder_lstm(decoder_inputs,initial_state = encoder_states)\n\ndecoder_dense = Dense(num_dec_tokens, activation='softmax')\ndecoder_ouputs = decoder_dense(decoder_ouputs)\n\nmodel = Model([encoder_inputs,decoder_inputs],decoder_ouputs)\nmodel.compile(optimizer='rmsprop',loss='categorical_crossentropy')\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T20:47:38.116201Z","iopub.execute_input":"2024-07-05T20:47:38.116433Z","iopub.status.idle":"2024-07-05T20:47:38.408552Z","shell.execute_reply.started":"2024-07-05T20:47:38.116395Z","shell.execute_reply":"2024-07-05T20:47:38.407775Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, None, 37)     0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            (None, None, 39)     0                                            \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   [(None, 256), (None, 301056      input_1[0][0]                    \n__________________________________________________________________________________________________\nlstm_2 (LSTM)                   [(None, None, 256),  303104      input_2[0][0]                    \n                                                                 lstm_1[0][1]                     \n                                                                 lstm_1[0][2]                     \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, None, 39)     10023       lstm_2[0][0]                     \n==================================================================================================\nTotal params: 614,183\nTrainable params: 614,183\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"h=model.fit([encoder_input_data,decoder_input_data],decoder_target_data\n         ,epochs = epochs,\n          batch_size = batch_size,\n          validation_split = 0.2\n         )\nmodel.save('s2s.h5')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T20:47:38.409703Z","iopub.execute_input":"2024-07-05T20:47:38.410012Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Train on 12122 samples, validate on 3031 samples\nEpoch 1/1000\n12122/12122 [==============================] - 12s 1ms/step - loss: 0.9706 - val_loss: 0.9327\nEpoch 2/1000\n12122/12122 [==============================] - 8s 629us/step - loss: 0.8959 - val_loss: 0.8642\nEpoch 3/1000\n12122/12122 [==============================] - 8s 629us/step - loss: 0.8392 - val_loss: 0.8196\nEpoch 4/1000\n12122/12122 [==============================] - 8s 641us/step - loss: 0.8071 - val_loss: 0.8078\nEpoch 5/1000\n12122/12122 [==============================] - 8s 626us/step - loss: 0.7877 - val_loss: 0.7854\nEpoch 6/1000\n12122/12122 [==============================] - 8s 628us/step - loss: 0.7675 - val_loss: 0.7576\nEpoch 7/1000\n12122/12122 [==============================] - 8s 634us/step - loss: 0.7499 - val_loss: 0.7504\nEpoch 8/1000\n12122/12122 [==============================] - 8s 641us/step - loss: 0.7376 - val_loss: 0.7359\nEpoch 9/1000\n12122/12122 [==============================] - 8s 623us/step - loss: 0.7247 - val_loss: 0.7505\nEpoch 10/1000\n12122/12122 [==============================] - 8s 629us/step - loss: 0.7210 - val_loss: 0.7139\nEpoch 11/1000\n12122/12122 [==============================] - 8s 632us/step - loss: 0.7029 - val_loss: 0.7084\nEpoch 12/1000\n12122/12122 [==============================] - 8s 636us/step - loss: 0.6835 - val_loss: 0.6944\nEpoch 13/1000\n12122/12122 [==============================] - 8s 622us/step - loss: 0.6744 - val_loss: 0.6977\nEpoch 14/1000\n12122/12122 [==============================] - 8s 626us/step - loss: 0.6630 - val_loss: 0.6750\nEpoch 15/1000\n12122/12122 [==============================] - 8s 628us/step - loss: 0.6555 - val_loss: 0.6767\nEpoch 16/1000\n12122/12122 [==============================] - 8s 638us/step - loss: 0.6377 - val_loss: 0.6565\nEpoch 17/1000\n12122/12122 [==============================] - 8s 626us/step - loss: 0.6286 - val_loss: 0.6388\nEpoch 18/1000\n12122/12122 [==============================] - 8s 636us/step - loss: 0.6129 - val_loss: 0.6357\nEpoch 19/1000\n12122/12122 [==============================] - 8s 628us/step - loss: 0.6027 - val_loss: 0.6188\nEpoch 20/1000\n12122/12122 [==============================] - 8s 637us/step - loss: 0.5838 - val_loss: 0.6118\nEpoch 21/1000\n12122/12122 [==============================] - 8s 623us/step - loss: 0.5719 - val_loss: 0.5965\nEpoch 22/1000\n12122/12122 [==============================] - 8s 625us/step - loss: 0.5565 - val_loss: 0.5828\nEpoch 23/1000\n12122/12122 [==============================] - 8s 630us/step - loss: 0.5430 - val_loss: 0.5816\nEpoch 24/1000\n12122/12122 [==============================] - 8s 631us/step - loss: 0.5327 - val_loss: 0.5680\nEpoch 25/1000\n12122/12122 [==============================] - 8s 632us/step - loss: 0.5210 - val_loss: 0.5646\nEpoch 26/1000\n12122/12122 [==============================] - 8s 623us/step - loss: 0.5117 - val_loss: 0.5555\nEpoch 27/1000\n12122/12122 [==============================] - 8s 629us/step - loss: 0.4988 - val_loss: 0.5479\nEpoch 28/1000\n12122/12122 [==============================] - 8s 629us/step - loss: 0.4880 - val_loss: 0.5454\nEpoch 29/1000\n12122/12122 [==============================] - 8s 645us/step - loss: 0.4784 - val_loss: 0.5397\nEpoch 30/1000\n12122/12122 [==============================] - 8s 627us/step - loss: 0.4699 - val_loss: 0.5291\nEpoch 31/1000\n12122/12122 [==============================] - 8s 631us/step - loss: 0.4594 - val_loss: 0.5455\nEpoch 32/1000\n12122/12122 [==============================] - 8s 633us/step - loss: 0.4495 - val_loss: 0.5210\nEpoch 33/1000\n12122/12122 [==============================] - 8s 651us/step - loss: 0.4396 - val_loss: 0.5215\nEpoch 34/1000\n12122/12122 [==============================] - 8s 634us/step - loss: 0.4303 - val_loss: 0.5174\nEpoch 35/1000\n12122/12122 [==============================] - 8s 630us/step - loss: 0.4224 - val_loss: 0.5137\nEpoch 36/1000\n12122/12122 [==============================] - 8s 636us/step - loss: 0.4138 - val_loss: 0.5169\nEpoch 37/1000\n12122/12122 [==============================] - 8s 648us/step - loss: 0.4112 - val_loss: 0.5393\nEpoch 38/1000\n12122/12122 [==============================] - 8s 629us/step - loss: 0.4239 - val_loss: 0.5106\nEpoch 39/1000\n12122/12122 [==============================] - 8s 626us/step - loss: 0.4038 - val_loss: 0.5138\nEpoch 40/1000\n12122/12122 [==============================] - 8s 627us/step - loss: 0.3889 - val_loss: 0.5068\nEpoch 41/1000\n12122/12122 [==============================] - 8s 646us/step - loss: 0.3749 - val_loss: 0.4853\nEpoch 42/1000\n12122/12122 [==============================] - 8s 631us/step - loss: 0.3623 - val_loss: 0.4765\nEpoch 43/1000\n12122/12122 [==============================] - 8s 628us/step - loss: 0.3510 - val_loss: 0.4739\nEpoch 44/1000\n12122/12122 [==============================] - 8s 626us/step - loss: 0.3397 - val_loss: 0.4667\nEpoch 45/1000\n12122/12122 [==============================] - 8s 641us/step - loss: 0.3301 - val_loss: 0.4689\nEpoch 46/1000\n12122/12122 [==============================] - 8s 631us/step - loss: 0.3198 - val_loss: 0.4655\nEpoch 47/1000\n12122/12122 [==============================] - 8s 624us/step - loss: 0.3102 - val_loss: 0.4565\nEpoch 48/1000\n12122/12122 [==============================] - 8s 630us/step - loss: 0.3009 - val_loss: 0.4610\nEpoch 49/1000\n12122/12122 [==============================] - 8s 645us/step - loss: 0.2925 - val_loss: 0.4534\nEpoch 50/1000\n12122/12122 [==============================] - 8s 638us/step - loss: 0.2834 - val_loss: 0.4480\nEpoch 51/1000\n12122/12122 [==============================] - 8s 626us/step - loss: 0.2744 - val_loss: 0.4469\nEpoch 52/1000\n12122/12122 [==============================] - 8s 627us/step - loss: 0.2660 - val_loss: 0.4447\nEpoch 53/1000\n12122/12122 [==============================] - 8s 643us/step - loss: 0.2576 - val_loss: 0.4458\nEpoch 54/1000\n12122/12122 [==============================] - 8s 628us/step - loss: 0.2497 - val_loss: 0.4462\nEpoch 55/1000\n12122/12122 [==============================] - 8s 624us/step - loss: 0.2418 - val_loss: 0.4544\nEpoch 56/1000\n12122/12122 [==============================] - 8s 623us/step - loss: 0.2343 - val_loss: 0.4515\nEpoch 57/1000\n12122/12122 [==============================] - 8s 642us/step - loss: 0.2270 - val_loss: 0.4445\nEpoch 58/1000\n12122/12122 [==============================] - 8s 631us/step - loss: 0.2194 - val_loss: 0.4464\nEpoch 59/1000\n12122/12122 [==============================] - 8s 629us/step - loss: 0.2123 - val_loss: 0.4503\nEpoch 60/1000\n12122/12122 [==============================] - 8s 632us/step - loss: 0.2048 - val_loss: 0.4486\nEpoch 61/1000\n12122/12122 [==============================] - 8s 654us/step - loss: 0.1992 - val_loss: 0.4472\nEpoch 62/1000\n12122/12122 [==============================] - 8s 625us/step - loss: 0.1924 - val_loss: 0.4488\nEpoch 63/1000\n12122/12122 [==============================] - 8s 630us/step - loss: 0.1852 - val_loss: 0.4507\nEpoch 64/1000\n12122/12122 [==============================] - 8s 620us/step - loss: 0.1803 - val_loss: 0.4509\nEpoch 65/1000\n12122/12122 [==============================] - 8s 640us/step - loss: 0.1733 - val_loss: 0.4601\nEpoch 66/1000\n12122/12122 [==============================] - 8s 634us/step - loss: 0.1670 - val_loss: 0.4682\nEpoch 67/1000\n12122/12122 [==============================] - 8s 622us/step - loss: 0.1621 - val_loss: 0.4569\nEpoch 68/1000\n12122/12122 [==============================] - 8s 626us/step - loss: 0.1566 - val_loss: 0.4632\nEpoch 69/1000\n12122/12122 [==============================] - 8s 625us/step - loss: 0.1508 - val_loss: 0.4711\nEpoch 70/1000\n12122/12122 [==============================] - 8s 651us/step - loss: 0.1457 - val_loss: 0.4748\nEpoch 71/1000\n12122/12122 [==============================] - 8s 627us/step - loss: 0.1406 - val_loss: 0.4725\nEpoch 72/1000\n12122/12122 [==============================] - 8s 623us/step - loss: 0.1357 - val_loss: 0.4760\nEpoch 73/1000\n12122/12122 [==============================] - 8s 629us/step - loss: 0.1310 - val_loss: 0.4741\nEpoch 74/1000\n12122/12122 [==============================] - 8s 646us/step - loss: 0.1262 - val_loss: 0.4892\nEpoch 75/1000\n12122/12122 [==============================] - 8s 623us/step - loss: 0.1216 - val_loss: 0.4856\nEpoch 76/1000\n12122/12122 [==============================] - 8s 627us/step - loss: 0.1176 - val_loss: 0.4918\nEpoch 77/1000\n12122/12122 [==============================] - 8s 629us/step - loss: 0.1131 - val_loss: 0.4931\nEpoch 78/1000\n12122/12122 [==============================] - 8s 652us/step - loss: 0.1089 - val_loss: 0.4928\nEpoch 79/1000\n12122/12122 [==============================] - 8s 635us/step - loss: 0.1049 - val_loss: 0.5045\nEpoch 80/1000\n12122/12122 [==============================] - 8s 645us/step - loss: 0.1012 - val_loss: 0.5129\nEpoch 81/1000\n12122/12122 [==============================] - 8s 653us/step - loss: 0.0980 - val_loss: 0.5233\nEpoch 82/1000\n12122/12122 [==============================] - 8s 664us/step - loss: 0.0941 - val_loss: 0.5214\nEpoch 83/1000\n12122/12122 [==============================] - 8s 637us/step - loss: 0.0904 - val_loss: 0.5177\nEpoch 84/1000\n12122/12122 [==============================] - 8s 631us/step - loss: 0.0878 - val_loss: 0.5288\nEpoch 85/1000\n12122/12122 [==============================] - 8s 644us/step - loss: 0.0843 - val_loss: 0.5374\nEpoch 86/1000\n12122/12122 [==============================] - 8s 660us/step - loss: 0.0812 - val_loss: 0.5342\nEpoch 87/1000\n12122/12122 [==============================] - 8s 646us/step - loss: 0.0783 - val_loss: 0.5365\nEpoch 88/1000\n12122/12122 [==============================] - 8s 650us/step - loss: 0.0752 - val_loss: 0.5455\nEpoch 89/1000\n12122/12122 [==============================] - 8s 654us/step - loss: 0.0722 - val_loss: 0.5485\nEpoch 90/1000\n12122/12122 [==============================] - 8s 668us/step - loss: 0.0703 - val_loss: 0.5536\nEpoch 91/1000\n12122/12122 [==============================] - 8s 651us/step - loss: 0.0673 - val_loss: 0.5639\nEpoch 92/1000\n12122/12122 [==============================] - 8s 645us/step - loss: 0.0651 - val_loss: 0.5660\nEpoch 93/1000\n12122/12122 [==============================] - 8s 642us/step - loss: 0.0628 - val_loss: 0.5665\nEpoch 94/1000\n12122/12122 [==============================] - 8s 667us/step - loss: 0.0602 - val_loss: 0.5678\nEpoch 95/1000\n12122/12122 [==============================] - 8s 646us/step - loss: 0.0588 - val_loss: 0.5708\nEpoch 96/1000\n12122/12122 [==============================] - 8s 647us/step - loss: 0.0554 - val_loss: 0.5806\nEpoch 97/1000\n12122/12122 [==============================] - 8s 649us/step - loss: 0.0543 - val_loss: 0.5838\nEpoch 98/1000\n12122/12122 [==============================] - 8s 655us/step - loss: 0.0526 - val_loss: 0.5932\nEpoch 99/1000\n12122/12122 [==============================] - 8s 648us/step - loss: 0.0504 - val_loss: 0.5880\nEpoch 100/1000\n12122/12122 [==============================] - 8s 643us/step - loss: 0.0490 - val_loss: 0.5931\nEpoch 101/1000\n12122/12122 [==============================] - 8s 646us/step - loss: 0.0471 - val_loss: 0.5966\nEpoch 102/1000\n12122/12122 [==============================] - 8s 658us/step - loss: 0.0449 - val_loss: 0.6149\nEpoch 103/1000\n 9472/12122 [======================>.......] - ETA: 1s - loss: 0.0433","output_type":"stream"}]},{"cell_type":"code","source":"plt.plot(h.history['loss'])\nplt.title('Model Loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_model = Model(encoder_inputs,encoder_states)\n\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h,decoder_state_input_c]\ndecoder_outputs,state_h,state_c = decoder_lstm(\n        decoder_inputs,initial_state = decoder_states_inputs\n)\ndecoder_states = [state_h,state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs] + decoder_states\n)\nencoder_model.save('encoder.h5')\ndecoder_model.save('decoder.h5')\n\ndef decode_sequence(input_seq):\n    states_value = encoder_model.predict(input_seq)\n    target_seq = np.zeros((1, 1, num_dec_tokens))\n    target_seq[0, 0, char2int['\\t']] = 1.\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = int2char[sampled_token_index]\n        decoded_sentence += sampled_char\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if (sampled_char == '\\n' or\n           len(decoded_sentence) > max_dec_len):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1, num_dec_tokens))\n        target_seq[0, 0, sampled_token_index] = 1.\n\n        # Update states\n        states_value = [h, c]\n\n    return decoded_sentence\n\nfor seq_index in range(10):\n    input_seq = encoder_input_data[seq_index: seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq)\n    print('-')\n    print('Wrong sentence:', input_texts[seq_index])\n    print('Corrected sentence:', decoded_sentence)\n    print('Ground Truth:',target_texts[seq_index])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}